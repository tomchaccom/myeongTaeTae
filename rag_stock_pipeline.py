import json
import os
from langchain.llms.base import LLM
import requests
import numpy as np
import pickle
import uuid
import pandas as pd

from dotenv import load_dotenv
load_dotenv()

def build_name_to_ticker_dict(
        kospi_path = "/opt/fastapi-app/fastapi/data/kospi_names.csv", 
        kosdaq_path="/opt/fastapi-app/fastapi/data/kosdaq_names.csv") -> dict:
    kospi_df = pd.read_csv(kospi_path)
    kosdaq_df = pd.read_csv(kosdaq_path)

    kospi_df["Market"] = "KS"
    kosdaq_df["Market"] = "KQ"

    all_df = pd.concat([kospi_df, kosdaq_df], ignore_index=True)

    return {
        row["Name"]: f'{str(row["Code"]).zfill(6)}.{row["Market"]}'
        for _, row in all_df.iterrows()
    }

# ì‚¬ìš©
name_to_ticker = build_name_to_ticker_dict()

API_KEY = os.getenv("HYPERCLOVA_API_KEY")
MODEL_NAME = "clir-emb-dolphin"
URL = f"https://clovastudio.stream.ntruss.com/testapp/v1/api-tools/embedding/{MODEL_NAME}"
os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGCHAIN_API_KEY")  # LangSmith API í‚¤

# --- í—¤ë” ì„¤ì • ---
headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
}


# --- HyperCLOVA LLM ì»¤ìŠ¤í…€ ì •ì˜ ---
class HyperCLOVA_LLM(LLM):
    api_key: str
    request_id: str
    system_prompt: str
    
    endpoint: str = "https://clovastudio.stream.ntruss.com/v3/chat-completions/HCX-005"

    @property
    def _llm_type(self) -> str:
        return "hyperclova"

    def _call(self, prompt: str, stop=None, **kwargs) -> str:
        headers = {
            'Authorization': self.api_key,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': self.request_id,
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }

        payload = {
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ],
            "topP": 0.8,
            "temperature": 0,
            "maxTokens": 256
        }

        response = requests.post(self.endpoint, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        return result["result"]["message"]["content"].strip('"').strip()



# system prompt: ì¢…ëª© ì¤„ì„ë§ ì¶”ì¶œìš© (JSON ë°°ì—´ ë°˜í™˜)
ABBR_SYSTEM_PROMPT = """
ë„ˆëŠ” ê¸ˆìœµê¶Œì—ì„œ ì“°ì´ëŠ” ì¢…ëª© ì•½ì–´ë¥¼ JSON ë°°ì—´ í˜•íƒœë¡œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” ì—­í• ì„ í•´.
ì¢…ëª©ëª…ì´ ì¤„ì„ë§ë¡œ ë‚˜ì™€ìˆëŠ” ê²½ìš°, ê·¸ ì¤„ì„ë§ì„ ë³€í™˜í•˜ì§€ ë§ê³  ì¶œë ¥ì— ë‹´ì•„ì¤˜ì•¼ í•´
ì¢…ëª©ëª…ì´ ì •í™•í•˜ê²Œ ëª…ì‹œë˜ì–´ ìˆëŠ” ê²½ìš°ì—ëŠ” [] í˜•íƒœì˜ ë¹ˆ ë°°ì—´ì„ ì¶œë ¥í•´

ì˜ˆ:
ì§ˆë¬¸ : ì–´ì œ ê¸°ì¤€ì‚¼ì „ì´ë‘ ì…€íŠ¸ ì¤‘ì— ë­ê°€ ë” ì¢…ê°€ê°€ ë†’ì•„?
ë‹µë³€ : ["ì‚¼ì „", "ì…€íŠ¸"]

"""

# HyperCLOVA ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (API í‚¤ëŠ” Bearer í¬í•¨í•´ì„œ ë„£ê¸°)
llm_abbr = HyperCLOVA_LLM(
    api_key=os.getenv("HYPERCLOVA_API_KEY"),
    request_id=f"abbr-{uuid.uuid4()}",
    system_prompt=ABBR_SYSTEM_PROMPT
)

# ì´ì œ í›„ë³´ ë‹¨ì–´ë¥¼ ë°›ì•„ì„œ ì„ë² ë”©í•˜ê¸° 

def get_embedding(text: str) -> np.ndarray | None:
    body = {"text": text}
    try:
        res = requests.post(URL, headers=headers, json=body)
        res.raise_for_status()
        embedding = res.json()["result"]["embedding"]
        return np.array(embedding)
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨: {e}")
        return None

# print(get_embedding(llm_abbr.invoke("ì—˜ì§€ì—”ì†” ìƒì¥íì§€ëì–´?")))

# stock_embeddings.pklì—ì„œ ì¢…ëª©ëª… ê°€ì ¸ì˜¤ê¸° 
# DenseRetriver(ì½”ì‚¬ì¸ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° )
def load_stock_embeddings(path="/opt/fastapi-app/fastapi/data/stock_embeddings2.pkl"):
    with open(path, "rb") as f:
        df = pickle.load(f)
    stock_names = df["Name"].tolist()
    stock_vecs = np.stack(df["Embedding"].values)
    return stock_names, stock_vecs

from sklearn.metrics.pairwise import cosine_similarity

def get_topk_similar_stocks(query_vec, stock_names, stock_vecs, top_k=5):
    scores = cosine_similarity([query_vec], stock_vecs)[0]
    top_indices = scores.argsort()[-top_k:][::-1]
    return [(stock_names[i], scores[i]) for i in top_indices]

llm_expander = HyperCLOVA_LLM(
    api_key=API_KEY,
    request_id=f"expand-{uuid.uuid4()}",
    system_prompt=
       """
        ë„ˆëŠ” ê¸ˆìœµê¶Œì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ì‹ ì€ì–´ë¥¼ JSON ë°°ì—´ í˜•íƒœë¡œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” ì—­í• ì„ í•´.
        ì£¼ì‹ ì€ì–´ëŠ” ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì–´ë ¤ìš´ í‘œí˜„ìœ¼ë¡œ, 'ì ì‚¼ë³‘' â†’ '3ì¼ê°„ ì–‘ë´‰', 'ì´ì•Œ' â†’ 'íˆ¬ìê¸ˆ'ì²˜ëŸ¼ êµ¬ì²´ì ì¸ ì£¼ì‹ ê°œë…ì„ ì€ì–´ë¡œ í‘œí˜„í•œ ê²ƒë“¤ì´ì•¼.

        ì§ˆë¬¸ì— ì€ì–´ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ê·¸ ì€ì–´ë¥¼ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë°°ì—´ì— ë‹´ì•„ì¤˜.
        ì€ì–´ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” [] í˜•íƒœì˜ ë¹ˆ ë°°ì—´ì„ ì¶œë ¥í•´.

        ì˜ˆ:
        ì§ˆë¬¸: "ì‚¼ì „ì „ìì˜ í‘ì‚¼ë³‘ ë° ë–¡ìƒ ì‹œì  ì–¸ì œì•¼?"
        ë‹µë³€: ["í‘ì‚¼ë³‘", "ë–¡ìƒ"]
         """
    
)
llm_slang_rewriter = HyperCLOVA_LLM(
    api_key=API_KEY,
    request_id=f"slang-rewrite-{uuid.uuid4()}",
    system_prompt="""
ë„ˆëŠ” ì£¼ì‹ ì „ë¬¸ê°€ë¡œì„œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í¬í•¨ëœ ì€ì–´ í‘œí˜„ì„ ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ë§ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë°”ê¿”ì£¼ëŠ” ì—­í• ì„ í•´.

- ì…ë ¥ìœ¼ë¡œëŠ” ì›ë³¸ ì§ˆë¬¸ê³¼ í•¨ê»˜ ì€ì–´ ë¦¬ìŠ¤íŠ¸ê°€ ì£¼ì–´ì§„ë‹¤.
- ì§ˆë¬¸ì—ì„œ í•´ë‹¹ ì€ì–´ë“¤ë§Œ ì •í™•íˆ ì°¾ì•„ì„œ ë°”ê¿”ì¤˜ì•¼ í•˜ë©°, ë‹¤ë¥¸ ë¶€ë¶„ì€ ê±´ë“œë¦¬ì§€ ë§ˆ.
- ì€ì–´ í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ ëª…í™•í•œ ì„¤ëª…ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ , ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ íë¦„ìœ¼ë¡œ ì¬ì‘ì„±í•´.
- ê²°ê³¼ëŠ” ìˆ˜ì •ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ê³ , ë‹¤ë¥¸ ë¶€ê°€ì„¤ëª…ì€ í•˜ì§€ ë§ˆ.

ì˜ˆì‹œ:
ì§ˆë¬¸: "SKí•˜ì´ ì§€ê¸ˆ ëˆŒë¦¼ëª© êµ¬ê°„ ê°™ì€ë°, ëª°ë¹µí•´ë„ ë ê¹Œ?"
ì€ì–´ ë¦¬ìŠ¤íŠ¸: ["ëˆŒë¦¼ëª©", "ëª°ë¹µ"]

ì¶œë ¥: "SKí•˜ì´ ì§€ê¸ˆ ë‹¨ê¸° ì¡°ì • êµ¬ê°„ ê°™ì€ë°, ì „ì•¡ íˆ¬ìí•´ë„ ë ê¹Œ?"
"""
)


# sparseRetriever
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer

class SparseRetriever:
    def __init__(self, stock_names):
        self.stock_names = stock_names
        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))  # ë¬¸ì ë‹¨ìœ„ë¡œ ë³€ê²½
        self.stock_tfidf_matrix = self.vectorizer.fit_transform(stock_names)

    def retrieve(self, query, top_k=5):
        query_vec = self.vectorizer.transform([query])
        scores = cosine_similarity(query_vec, self.stock_tfidf_matrix).flatten()
        top_indices = scores.argsort()[-top_k:][::-1]
        return [(self.stock_names[i], scores[i]) for i in top_indices]

def normalize_scores(results):
    """
    results: [(name, score), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
    scoreë“¤ì„ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ì—¬ ë°˜í™˜
    """
    scores = [score for _, score in results]
    min_score, max_score = min(scores), max(scores)
    
    if max_score - min_score == 0:
        # ëª¨ë‘ ì ìˆ˜ê°€ ê°™ìœ¼ë©´ 1.0ìœ¼ë¡œ í†µì¼
        return [(name, 1.0) for name, _ in results]
    
    normalized = [
        (name, (score - min_score) / (max_score - min_score))
        for name, score in results
    ]
    return normalized


def ensemble_results(sparse_results, dense_results, weight_sparse=0.8, weight_dense=0.2):
    all_names = set([name for name, _ in sparse_results] + [name for name, _ in dense_results])
    sparse_dict = dict(sparse_results)
    dense_dict = dict(dense_results)

    # default ê°’: ê°€ì¥ ë‚®ì€ ì ìˆ˜ ë˜ëŠ” í‰ê·  ì ìˆ˜ (ë” ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜)
    default_sparse = min(sparse_dict.values()) if sparse_dict else 0
    default_dense = min(dense_dict.values()) if dense_dict else 0

    ensemble_scores = {
        name: weight_sparse * sparse_dict.get(name, default_sparse) +
              weight_dense * dense_dict.get(name, default_dense)
        for name in all_names
    }

    return sorted(ensemble_scores.items(), key=lambda x: x[1], reverse=True)


def rerank_candidates_with_clovax(llm: HyperCLOVA_LLM, question: str, candidates: list[str]) -> str:
    """
    question: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
    candidates: ì•™ìƒë¸”ë¡œ ë½‘ì€ í›„ë³´ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['ì‚¼ì„±ì „ì', 'ì…€íŠ¸ë¦¬ì˜¨', 'ì¹´ì¹´ì˜¤'])
    
    ë¦¬í„´ê°’: ìµœì¢…ì ìœ¼ë¡œ í›„ë³´ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì¤‘ LLMì´ íŒë‹¨í•œ ê°€ì¥ ì í•©í•œ ì¢…ëª©ëª…
    """
    # í›„ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ì˜ˆì˜ê²Œ ë³€í™˜
    candidates_str = ", ".join(candidates)
    
    prompt = f"""
    ë„ˆëŠ” í•œêµ­ ì£¼ì‹ ì „ë¬¸ê°€ì•¼.
    ë‹¤ìŒ ì§ˆë¬¸ì— ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì¢…ëª© í•˜ë‚˜ë¥¼ ê³ ë¥´ê³ , ê·¸ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜.
    
    ì§ˆë¬¸: "{question}"
    í›„ë³´ ì¢…ëª©ë“¤ (ì¢…ëª©ëª… ê¸°ì¤€): {candidates_str}

    ê°€ì¥ ì í•©í•œ ì¢…ëª©ëª…ì„ ì¶œë ¥ë§Œ í•´ì¤˜.\
    """
    
    result = llm.invoke(prompt)
    
    # ê²°ê³¼ì—ì„œ ì¢…ëª©ëª…ë§Œ ê¹”ë”í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” í›„ì²˜ë¦¬ (í•„ìš”í•˜ë©´ ê°•í™” ê°€ëŠ¥)
    chosen_stock = result.strip().split("\n")[0]
    
    return chosen_stock

# 1) í´ë¡œë°” X LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm_clovax = HyperCLOVA_LLM(
    api_key=API_KEY,
    request_id=f"clovax-{uuid.uuid4()}",
    system_prompt="ë„ˆëŠ” í•œêµ­ ì£¼ì‹ ì „ë¬¸ê°€ì•¼. ì§ˆë¬¸ê³¼ í›„ë³´ ì¢…ëª©ì„ ë³´ê³  ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì¢…ëª©ì„ ê³ ë¥´ëŠ” ì—­í• ì„ ìˆ˜í–‰í•´."
)

rewrite_llm = HyperCLOVA_LLM(
    api_key=API_KEY,
    request_id=f"rewrite-{uuid.uuid4()}",
    system_prompt="ë„ˆëŠ” ê¸ˆìœµ ë„ë©”ì¸ì˜ í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ ì „ë¬¸ê°€ì•¼. ì§ˆë¬¸ ë‚´ì˜ ì¤„ì„ë§ì„ ì¢…ëª© ì „ì²´ ì´ë¦„ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì¹˜í™˜í•˜ëŠ” ì—­í• ì„ í•´."
)
def rewrite_question_with_ticker_code(llm: HyperCLOVA_LLM, original_question: str, tickers: list[str]) -> str:
    """
    original_question: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ (ì˜ˆ: "ì‚¼ì „ ìš”ì¦˜ ì£¼ê°€ ì–´ë•Œ?")
    tickers: ë¦¬ë­í‚¹ì„ í†µí•´ ì„ ì •ëœ ì •ì‹ ì¢…ëª©ì½”ë“œ (ì˜ˆ: ["005930.KS"])
    
    return: ì¢…ëª© ì¤„ì„ë§ë¥¼ ì¢…ëª©ì½”ë“œë¡œ ë°”ê¾¼ ì¬ì‘ì„± ì§ˆë¬¸ ë¬¸ìì—´
    """
    tickers_str = ", ".join(tickers)
    
    prompt = f"""
    ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ê³¼, ê·¸ ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì¢…ëª© ì¤„ì„ë§ì„ ì •ì‹ ì¢…ëª©ì½”ë“œë¡œ ë°”ê¾¸ê¸° ìœ„í•œ ì°¸ê³  ëª©ë¡ì´ì•¼.

    ì›ë³¸ ì§ˆë¬¸: "{original_question}"
    ì •ì‹ ì¢…ëª©ì½”ë“œ ëª©ë¡: {tickers_str}

    ì§ˆë¬¸ì— í¬í•¨ëœ ì¢…ëª© ì¤„ì„ë§ì„ ì•„ë˜ ëª©ë¡ì— ìˆëŠ” ì¢…ëª©ì½”ë“œë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë°”ê¿”ì„œ ë‹¤ì‹œ ì‘ì„±í•´ì¤˜. ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ¬ì›Œì•¼ í•˜ê³ ,ë°˜ë“œì‹œ ì¢…ëª©ì½”ë“œê°€ ëª¨ë‘ ë°˜ì˜ë¼ì•¼ í•´.
    
    ë°˜ë“œì‹œ ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥í•´
    
    ì˜ˆì‹œ ì¶œë ¥: ì¼„ë‹¬ìŠ¤í€˜ì–´ë¦¬ì¸ (365550) ì§€ê¸ˆ ë§¤ìˆ˜í•´ë„ ê´œì°®ì„ê¹Œìš”?
    """

    rewritten = llm.invoke(prompt).strip()
    return rewritten


def preprocess_question(user_question: str, stock_names, stock_vecs) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        user_question: ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸
        stock_names: ì¢…ëª©ëª… ë¦¬ìŠ¤íŠ¸
        stock_vecs: ì¢…ëª© ì„ë² ë”© ë²¡í„°
    
    Returns:
        ì „ì²˜ë¦¬ëœ ì§ˆë¬¸ ë¬¸ìì—´
    """
    # ì¢…ëª© ì•½ì–´ ì¶”ì¶œ
    try:
        llm_response = llm_abbr.invoke(user_question)
        #print("[LLM ì•½ì–´ ì¶”ì¶œ ì‘ë‹µ]", llm_response)
        query_term = json.loads(llm_response)
    except Exception as e:
        print("[LLM ì•½ì–´ ì¶”ì¶œ íŒŒì‹± ì‹¤íŒ¨]", e)
        query_term = []
    
    # ì¶”ì¶œëœ ì•½ì–´ê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ì€ì–´ ë³€í™˜ ë‹¨ê³„ë¡œ ë„˜ì–´ê°
    if not query_term:
        print("[ì•½ì–´ ì¶”ì¶œ ì—†ìŒ] ë°”ë¡œ ì€ì–´ ë³€í™˜ ë‹¨ê³„ë¡œ ì§„í–‰")
        rewritten = user_question
    else:
        answer = []
        for word in query_term:
            query_vec = get_embedding(word)
            dense_results = get_topk_similar_stocks(query_vec, stock_names, stock_vecs, top_k=5)
            top_name, top_score = dense_results[0]
            
            if top_score >= 1.0:
                ticker = name_to_ticker.get(top_name, top_name)
                answer.append(ticker)  # ë°”ë¡œ answerì— ì¶”ê°€
                continue
            else:
                sparse_retriever = SparseRetriever(stock_names)
                sparse_results = sparse_retriever.retrieve(word, top_k=5)
                dense_norm = normalize_scores(dense_results)
                sparse_norm = normalize_scores(sparse_results)
                final_results = ensemble_results(sparse_norm, dense_norm)
                candidate_stocks = [name for name, score in final_results]
                best_stock = rerank_candidates_with_clovax(llm_clovax, user_question, candidate_stocks[:5])
                ticker = name_to_ticker.get(best_stock, best_stock)
                answer.append(ticker)
        
        rewritten = rewrite_question_with_ticker_code(rewrite_llm, user_question, answer)
        print(rewritten)
    
    # ì€ì–´ ë³€í™˜ ë‹¨ê³„
    slang_list_raw = (llm_expander.invoke(rewritten))
    try:
        slang_list = json.loads(slang_list_raw)
        if not isinstance(slang_list, list):
            raise ValueError("ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹˜")
    except Exception as e:
        print("[ì€ì–´ íŒŒì‹± ì‹¤íŒ¨] ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ:", slang_list_raw)
        slang_list = []
    
    slang_str = ", ".join(slang_list)
    prompt = f"""
    ì›ë³¸ ì§ˆë¬¸: "{rewritten}"
    ì£¼ì‹ ì€ì–´ ëª©ë¡: {slang_str}
    ìœ„ ì§ˆë¬¸ì„ ì£¼ì‹ ì€ì–´ë¥¼ ìì—°ì–´ë¡œ í’€ì–´ì“°ëŠ” ë¬¸ì¥ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ì¤˜.
    """
    preprocessed_question = llm_slang_rewriter.invoke(prompt)
    return preprocessed_question


if __name__ == "__main__":


    import time
    start_time = time.time() 

    # ì¢…ëª© ë²¡í„° ë¶ˆëŸ¬ì˜¤ê¸°
    stock_names, stock_vecs = load_stock_embeddings("stock_embeddings2.pkl")

    # ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸ì€ ë³„ë„ë¡œ ì¤„ì„ë§ ì¶”ì¶œ ì „ì— ë°›ëŠ”ë‹¤ê³  ê°€ì •
    user_question = "ì¼„ë‹¬ìŠ¤í€˜ì–´ë¦¬ì¸  ì§€ê¸ˆ ì‚¬ë„ ë¼?"

    # ì‚¬ìš©ì ì§ˆë¬¸ í›„ë³´ ë‹¨ì–´ ì…ë ¥ (ì¤„ì„ë§ ì¶”ì¶œì€ ë³„ë„ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬ ì˜ˆì •)
    # query_term = json.loads(llm_abbr.invoke(user_question))
    # print(query_term)

    # ì¶”ì¶œëœ ë‹¨ì–´ì˜ ê°’ìœ¼ë¡œ ê²€ìƒ‰ê¸°ë¥¼ ëŒë ¸ëŠ”ë° ê°’ì´ 1ì´ìƒì´ ë‚˜ì˜¤ë©´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì§„í–‰í•  í•„ìš”ê°€ ì—†ë‹¤. 
    # ê·¸ëŸ¬ë©´ return ì„ í•˜ë©´ ë ë ¤ë‚˜?
    
    # answer = []
    # for word in query_term:
    #     # DenseRetriever: ì§ˆë¬¸ í›„ë³´ ë‹¨ì–´ ì„ë² ë”© í›„ ìœ ì‚¬ë„ ê²€ìƒ‰
    #     query_vec = (get_embedding(word))
    
    #     dense_results = get_topk_similar_stocks(query_vec, stock_names, stock_vecs, top_k=5)

    #     top_name, top_score = dense_results[0]

    #     if top_score >= 1.0:
    #         print("ì´ë¯¸ ì¢…ëª© í’€ë„¤ì„ì…ë‹ˆë‹¤")
    #         continue
    #     # elif top_score < 0.3:
    #     #     print("íœ´ë¨¼ ì¸í„°ëŸ½íŠ¸")
    #     #     exit()
    #     else:
    #         # SparseRetriever: ì§ˆë¬¸ í›„ë³´ ë‹¨ì–´ë¡œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰
    #         sparse_retriever = SparseRetriever(stock_names)
    #         sparse_results = sparse_retriever.retrieve(word, top_k=5)

    #         # ì •ê·œí™” ìˆ˜í–‰ (normalize_scores í•¨ìˆ˜ëŠ” ë³„ë„ ì •ì˜ í•„ìš”)
    #         dense_norm = normalize_scores(dense_results)
    #         sparse_norm = normalize_scores(sparse_results)

    #         # ì•™ìƒë¸” ì ìˆ˜ ê³„ì‚° (ensemble_results í•¨ìˆ˜ë„ ë³„ë„ ì •ì˜ í•„ìš”)
    #         final_results = ensemble_results(sparse_norm, dense_norm)

    #         # í›„ë³´ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ìµœì¢… ë­í‚¹ìš©)
    #         candidate_stocks = [name for name, score in final_results]

    #         # ë¦¬ë­í‚¹ ìˆ˜í–‰ (rerank_candidates_with_clovax í•¨ìˆ˜ëŠ” ë³„ë„ ì •ì˜ë¨)
    #         best_stock = rerank_candidates_with_clovax(llm_clovax, user_question, candidate_stocks[:5])

    #         # ê²°ê³¼ ì¶œë ¥
    #         # print(f"\nğŸ” Dense ê²°ê³¼: {dense_results}")
    #         # print(f"ğŸ” Sparse ê²°ê³¼: {sparse_results}")
    #         # print(f"ğŸ† ì•™ìƒë¸” Top ì¢…ëª©: {final_results}")
    #         # print(f"ğŸ¯ ìµœì¢… ë¦¬ë­í‚¹ëœ ì¢…ëª©: {best_stock}")
    #         answer.append(best_stock)
    #         # print(answer)

            
        
    #     rewritten = rewrite_question_with_full_stock_names(rewrite_llm, user_question, answer)
    #     # print("âœï¸ ì¬ì‘ì„±ëœ ì§ˆë¬¸:", rewritten)

    #     slang_list_raw = (llm_expander.invoke(rewritten))

    #     try:
    #         slang_list = json.loads(slang_list_raw)
    #     except json.JSONDecodeError:
    #         slang_list = eval(slang_list_raw)

    #     # slang_listê°€ ["ëˆŒë¦¼ëª©", "ëª°ë¹µ"] ê°™ì€ ë¦¬ìŠ¤íŠ¸ë¼ë©´
    #     slang_str = ", ".join(slang_list)

    #     prompt = f"""
    #     ì›ë³¸ ì§ˆë¬¸: "{rewritten}"
    #     ì£¼ì‹ ì€ì–´ ëª©ë¡: {slang_str}

    #     ìœ„ ì§ˆë¬¸ì„ ì£¼ì‹ ì€ì–´ë¥¼ ìì—°ì–´ë¡œ í’€ì–´ì“°ëŠ” ë¬¸ì¥ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ì¤˜.
    #     """

    #     rewritten_expanded = llm_slang_rewriter.invoke(prompt)
    #     print("âœï¸ ì¬ì‘ì„±ëœ ì§ˆë¬¸:", rewritten_expanded)

    #     end_time = time.time()  # ëë‚˜ëŠ” ì‹œê°„ ê¸°ë¡
    #     print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")

    preprocessed_question = preprocess_question(user_question, stock_names, stock_vecs)
    print("âœ… ì „ì²˜ë¦¬ëœ ì§ˆë¬¸:", preprocessed_question)

    end_time = time.time()  # ëë‚˜ëŠ” ì‹œê°„ ê¸°ë¡
    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")